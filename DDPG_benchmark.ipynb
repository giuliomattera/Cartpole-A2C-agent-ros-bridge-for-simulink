{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "eab69cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "\n",
    "#Simulation configuration\n",
    "MAX_EPISODE = 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9555ee63",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPG():\n",
    "\n",
    "    def __init__(self, GAMMA=0.99, EFFORT=1, CLR = 1e-3, ALR = 1e-4, BATCH = 8, TAU = 0.005,\n",
    "    STD = 0.2, DT = 1e-3, THETA = 0.15):\n",
    "        super(DDPG, self).__init__()\n",
    "        self.num_states = 1\n",
    "        self.num_actions = 1\n",
    "        self.gamma = GAMMA\n",
    "        self.max_effort = EFFORT\n",
    "        self.CRITIC_LR = CLR\n",
    "        self.ACTOR_LR = ALR\n",
    "        self.BATCH = BATCH\n",
    "        self.TAU = TAU\n",
    "        self.THETA = THETA\n",
    "        self.DT = DT\n",
    "        self.NMEAN = np.zeros(self.num_actions)\n",
    "        self.STD = np.float(STD)*np.ones(self.num_actions)\n",
    "\n",
    "    def Actor(self):\n",
    "\n",
    "        initializer = tf.random_uniform_initializer(minval=-0.003, maxval=0.003)\n",
    "\n",
    "        input = tf.keras.layers.Input(shape=(None,self.num_states))\n",
    "        \n",
    "        hidden = tf.keras.layers.Dense(400, \n",
    "        activation=tf.keras.layers.ReLU(), \n",
    "        kernel_initializer= initializer)(input)\n",
    "        \n",
    "        hidden = tf.keras.layers.Dense(300, \n",
    "        activation=tf.keras.layers.ReLU(), \n",
    "        kernel_initializer= initializer)(hidden)\n",
    "\n",
    "        outputs = tf.keras.layers.Dense(\n",
    "            self.num_actions, activation=\"tanh\")(hidden)\n",
    "        \n",
    "        outputs = tf.keras.layers.experimental.preprocessing.Rescaling(self.max_effort)(outputs)    \n",
    "        \n",
    "        return tf.keras.Model(input, outputs)\n",
    "\n",
    "    def Critic(self):\n",
    "\n",
    "        initializer = tf.random_uniform_initializer(minval=-0.003, maxval=0.003)\n",
    "\n",
    "        # State as input\n",
    "        state_input = tf.keras.layers.Input(shape=(self.num_states) )\n",
    "        \n",
    "        state_out_critic = tf.keras.layers.Dense(400, \n",
    "        activation=tf.keras.layers.ReLU(),\n",
    "         kernel_initializer= initializer)(state_input)\n",
    "        \n",
    "        state_out_critic = tf.keras.layers.Dense(300, \n",
    "        activation=tf.keras.layers.ReLU(),\n",
    "         kernel_initializer= initializer)(state_out_critic)\n",
    "\n",
    "        # Action as input\n",
    "        action_input = tf.keras.layers.Input(shape=(self.num_actions))\n",
    "\n",
    "        action_out_critic = tf.keras.layers.Dense(300,\n",
    "        activation=tf.keras.layers.ReLU(),\n",
    "         kernel_initializer= initializer)(action_input)\n",
    "\n",
    "        # Concatening 2 networks\n",
    "        concat = tf.keras.layers.Concatenate()(\n",
    "            [state_out_critic, action_out_critic]\n",
    "            )\n",
    "\n",
    "        out = tf.keras.layers.Dense(128, \n",
    "            activation=tf.keras.layers.ReLU(),\n",
    "            kernel_initializer= initializer)(concat)\n",
    "        \n",
    "        # Predicted Q(s,a)\n",
    "        outputs = tf.keras.layers.Dense(1)(out)\n",
    "\n",
    "        return tf.keras.Model([state_input, action_input], outputs)\n",
    "\n",
    "    def initializer(self):\n",
    "        global n\n",
    "\n",
    "        n = 0\n",
    "        actor_model = self.Actor()\n",
    "        critic_model = self.Critic()\n",
    "\n",
    "        target_actor = self.Actor()\n",
    "        target_critic = self.Critic()\n",
    "\n",
    "        critic_lr = self.CRITIC_LR\n",
    "        actor_lr = self.ACTOR_LR\n",
    "\n",
    "        critic_optimizer = tf.keras.optimizers.Adam(critic_lr )\n",
    "        actor_optimizer = tf.keras.optimizers.Adam(actor_lr)\n",
    "\n",
    "\n",
    "        models = [critic_model, actor_model, target_critic, target_actor]\n",
    "        optimizers = [critic_optimizer, actor_optimizer]\n",
    "\n",
    "        return models, optimizers \n",
    "\n",
    "    def update_target(self, target_weights, weights):\n",
    "            for (a, b) in zip(target_weights, weights):\n",
    "                a.assign(b * self.TAU + a * (1 - self.TAU))   \n",
    "     \n",
    "    def update(self, models, optimizers, state, action, reward, next_state, next_action, batch):\n",
    "\n",
    "        state = np.array(state, dtype = np.float32)\n",
    "\n",
    "        if batch == 1:\n",
    "            state = state.reshape(len(state),self.num_states)\n",
    "        else:\n",
    "            state = state.reshape(1,4)\n",
    "        state = tf.convert_to_tensor(state, dtype=tf.float32)\n",
    "\n",
    "        next_state = np.array(next_state)\n",
    "        if batch == 1:\n",
    "            next_state = next_state.reshape(len(next_state),self.num_states)\n",
    "        else:\n",
    "            next_state = next_state.reshape(1,4)\n",
    "\n",
    "        next_state = tf.convert_to_tensor(next_state, dtype=tf.float32)\n",
    "        \n",
    "        action = np.float32(action)\n",
    "        if batch == 1:\n",
    "            action = action.reshape(len(action),self.num_actions)\n",
    "        else:\n",
    "            action = tf.expand_dims(action, 0)\n",
    "        action = tf.convert_to_tensor(action, dtype=tf.float32)\n",
    "\n",
    "        next_action = np.float32(next_action)\n",
    "        if batch == 1:\n",
    "            next_action = next_action.reshape(len(next_action),self.num_actions)\n",
    "        else:\n",
    "            next_action = tf.expand_dims(next_action, 0)\n",
    "        next_action = tf.convert_to_tensor(next_action, dtype=tf.float32)\n",
    "        \n",
    "        \n",
    "        reward = tf.expand_dims(np.float32(reward), 0)\n",
    "        reward = tf.convert_to_tensor(reward, dtype=tf.float32)\n",
    "\n",
    "        with tf.GradientTape() as gradient:\n",
    "                        \n",
    "            target_actions = models[3](next_state, training = True)\n",
    "\n",
    "            critic_q = models[0]([state, action], training=True)  # Q(s,a)\n",
    "\n",
    "            y = reward + self.gamma * \\\n",
    "                models[2]([next_state, target_actions],\n",
    "                             training=True)  \n",
    "            \n",
    "            critic_loss = tf.keras.losses.mean_squared_error(y_true=y, y_pred=critic_q)\n",
    "            \n",
    "            critic_grad = gradient.gradient(\n",
    "                critic_loss, models[0].trainable_variables)\n",
    "            \n",
    "            critic_grad = [(tf.clip_by_value(grad, -1,1)) for grad in critic_grad]\n",
    "\n",
    "            optimizers[0].apply_gradients(\n",
    "                zip(critic_grad, models[0].trainable_variables))\n",
    "\n",
    "        with tf.GradientTape() as gradient:\n",
    "\n",
    "            actions = models[1](state, training=True)\n",
    "\n",
    "            critic_q = models[0]([state, actions], training=True)\n",
    "\n",
    "            actor_loss = tf.math.reduce_mean(-critic_q)\n",
    "\n",
    "            actor_grad = gradient.gradient(\n",
    "                actor_loss, models[1].trainable_variables)\n",
    "            \n",
    "            actor_grad = [(tf.clip_by_value(grad, -1,1)) for grad in actor_grad]\n",
    "            \n",
    "            optimizers[1].apply_gradients(\n",
    "                zip(actor_grad, models[1].trainable_variables))\n",
    "\n",
    "        self.update_target(models[3].variables, models[1].variables)\n",
    "        self.update_target(models[2].variables, models[0].variables)\n",
    "\n",
    "        return critic_loss, actor_loss\n",
    "\n",
    "    def UONoise(self, bn):\n",
    "        global n\n",
    "        n = bn + self.THETA *(self.NMEAN - bn)*self.DT + self.STD + np.sqrt(self.DT) * np.random.normal(size=self.NMEAN.shape)\n",
    "\n",
    "        return n\n",
    "\n",
    "    def make_action(self, actor_model, state):\n",
    "        global n\n",
    "        #s_nn = np.array(state, dtype=np.float32)\n",
    "        #s_nn = tf.expand_dims(s_nn, 0) #prepare state for NN \n",
    "        n = self.UONoise(n)\n",
    "\n",
    "        return np.float32(actor_model(state) + n)\n",
    "\n",
    "    def train(self, models, optimizers, total_trajectory):\n",
    "\n",
    "        num_sample = len(total_trajectory)\n",
    "        total_rew, atl, ctl = [], [], []\n",
    "\n",
    "        if num_sample > 4:\n",
    "            if num_sample < self.BATCH:\n",
    "                print('Training without batch beacuse num of samples < batch size')\n",
    "                for i in range(num_sample):\n",
    "                    print('Learning from sample ', i+1)\n",
    "\n",
    "                    closs, aloss = self.update(models, optimizers, \n",
    "                                            total_trajectory[i][0],\n",
    "                                            total_trajectory[i][1], \n",
    "                                            total_trajectory[i][2],\n",
    "                                            total_trajectory[i][3], \n",
    "                                            total_trajectory[i][4],\n",
    "                                            0)\n",
    "\n",
    "            else:\n",
    "                for mb in range(num_sample//self.BATCH):\n",
    "                    mini_state0 = []\n",
    "                    mini_action0 = []\n",
    "                    mini_state1 = []\n",
    "                    mini_action1 = []\n",
    "                    mini_reward = []\n",
    "                    for sample in range(self.BATCH):\n",
    "                        mini_state0.append(total_trajectory[mb*self.BATCH:self.BATCH*(mb+1)+1][sample][0])\n",
    "                        mini_action0.append(total_trajectory[mb*self.BATCH:self.BATCH*(mb+1)+1][sample][1])\n",
    "                        mini_reward.append(total_trajectory[mb*self.BATCH:self.BATCH*(mb+1)+1][sample][2])\n",
    "                        mini_state1.append(total_trajectory[mb*self.BATCH:self.BATCH*(mb+1)+1][sample][3])\n",
    "                        mini_action1.append(total_trajectory[mb*self.BATCH:self.BATCH*(mb+1)+1][sample][4])\n",
    "                        \n",
    "                    closs, aloss = self.update(models, optimizers,\n",
    "                                                mini_state0,\n",
    "                                                mini_action0, \n",
    "                                                mini_reward,\n",
    "                                                mini_state1, \n",
    "                                                mini_action1,\n",
    "                                                1)\n",
    "                    \n",
    "                    total_rew.append(sum(mini_reward)/len(mini_reward))\n",
    "                    atl.append(aloss)\n",
    "                    ctl.append(closs)\n",
    "                    \n",
    "            avg_reward = np.mean(total_rew)\n",
    "            avg_atl = np.mean(atl)\n",
    "            avg_ctl = np.mean(ctl)\n",
    "            print(\"Avg reward{} , Avg actor loss is {}, Avg critic loss is {}\".format(avg_reward, avg_atl, avg_ctl))\n",
    "        else:\n",
    "            print('[WARNING] Few samples acquired. Skipping training phase. ')\n",
    "        return avg_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b0311d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Global variables initialization...\n",
      "[INFO] Simulation is finish. Learning for episode  1  / 1000\n",
      "Avg reward-3.740112543106079 , Avg actor loss is 0.10846234112977982, Avg critic loss is 21.854019165039062\n",
      "[INFO] Simulation is finish. Learning for episode  2  / 1000\n",
      "Avg reward-5.199722766876221 , Avg actor loss is 0.3142763376235962, Avg critic loss is 15.242389678955078\n",
      "[INFO] Simulation is finish. Learning for episode  3  / 1000\n",
      "Avg reward-6.1664719581604 , Avg actor loss is 0.16486269235610962, Avg critic loss is 17.281787872314453\n",
      "[INFO] Simulation is finish. Learning for episode  4  / 1000\n",
      "Avg reward-6.754498481750488 , Avg actor loss is 0.0770951583981514, Avg critic loss is 15.352983474731445\n",
      "[INFO] Simulation is finish. Learning for episode  5  / 1000\n",
      "Avg reward-7.026749134063721 , Avg actor loss is 0.07093824446201324, Avg critic loss is 16.000022888183594\n",
      "[INFO] Simulation is finish. Learning for episode  6  / 1000\n",
      "Avg reward-7.100833415985107 , Avg actor loss is 0.0689413994550705, Avg critic loss is 14.648345947265625\n",
      "[INFO] Simulation is finish. Learning for episode  7  / 1000\n",
      "Avg reward-7.137517929077148 , Avg actor loss is 0.06833035498857498, Avg critic loss is 16.91292953491211\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from stable_baselines.common import make_vec_env\n",
    "\n",
    "envs = make_vec_env('Pendulum-v1')\n",
    "\n",
    "print('[INFO] Global variables initialization...')\n",
    "\n",
    "total_trajectory, tr = [], []\n",
    "\n",
    "agents = DDPG()\n",
    "agents.num_states = env.observation_space.shape[0]\n",
    "agents.num_actions = env.action_space.shape[0]\n",
    "agents.max_effort = 2\n",
    "agents.gamma = 0.98\n",
    "agents.std = 0.1\n",
    "agents.ALC = 0.001\n",
    "agents.CLR = 0.002\n",
    "agents.BATCH = 16\n",
    "models, optimizers = agents.initializer()\n",
    "\n",
    "\n",
    "s0 = [0]*agents.num_states\n",
    "a0 = np.array(0, dtype = np.float32)\n",
    "r = np.array(0, dtype = np.float32)\n",
    "G_t = np.array(0, dtype = np.float32)\n",
    "i, j, t, started, avg_rew, episode, step, done = 0 , 0, 0, 0, 0, 0, 0, False\n",
    "states, actions, traj, total_trajectory = [], [], [], []\n",
    "\n",
    "\n",
    "while episode < MAX_EPISODE:\n",
    "    state = envs.reset()\n",
    "    while done == False:\n",
    "        action = agents.make_action(models[1], state)\n",
    "        next_state, reward, done, _ = envs.step(action)\n",
    "        next_action = agents.make_action(models[1], next_state)\n",
    "        total_trajectory.append([state, action, reward, next_state, next_action])\n",
    "        envs.render()\n",
    "        step += 1\n",
    "        if done:\n",
    "            print('[INFO] Simulation is finish. Learning for episode ', episode+1, ' /', MAX_EPISODE)\n",
    "            agents.train(models, optimizers, total_trajectory)\n",
    "            done = False\n",
    "            episode += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bd1f5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa5962f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
