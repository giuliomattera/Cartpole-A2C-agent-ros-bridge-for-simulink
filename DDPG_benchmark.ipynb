{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65ba4d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "\n",
    "#Simulation configuration\n",
    "MAX_EPISODE = 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d54257f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPG():\n",
    "\n",
    "    def __init__(self, GAMMA=0.99, EFFORT=1, CLR = 1e-3, ALR = 1e-4, BATCH = 8, TAU = 0.005,\n",
    "    STD = 0.2, DT = 1e-3, THETA = 0.15):\n",
    "        super(DDPG, self).__init__()\n",
    "        self.num_states = 1\n",
    "        self.num_actions = 1\n",
    "        self.gamma = GAMMA\n",
    "        self.max_effort = EFFORT\n",
    "        self.CRITIC_LR = CLR\n",
    "        self.ACTOR_LR = ALR\n",
    "        self.BATCH = BATCH\n",
    "        self.TAU = TAU\n",
    "        self.THETA = THETA\n",
    "        self.DT = DT\n",
    "        self.NMEAN = np.zeros(self.num_actions)\n",
    "        self.STD = np.float(STD)*np.ones(self.num_actions)\n",
    "\n",
    "    def Actor(self):\n",
    "\n",
    "        initializer = tf.random_uniform_initializer(minval=-0.002, maxval=0.002)\n",
    "\n",
    "        input = tf.keras.layers.Input(shape=(None,self.num_states))\n",
    "        \n",
    "        hidden = tf.keras.layers.Dense(300, \n",
    "        activation=tf.keras.layers.ReLU(), \n",
    "        kernel_initializer= initializer)(input)\n",
    "        \n",
    "        hidden = tf.keras.layers.Dense(100, \n",
    "        activation=tf.keras.layers.ReLU(), \n",
    "        kernel_initializer= initializer)(hidden)\n",
    "\n",
    "        outputs = tf.keras.layers.Dense(\n",
    "            self.num_actions, activation=\"tanh\")(hidden)\n",
    "        \n",
    "        outputs = tf.keras.layers.experimental.preprocessing.Rescaling(self.max_effort)(outputs)    \n",
    "        \n",
    "        return tf.keras.Model(input, outputs)\n",
    "\n",
    "    def Critic(self):\n",
    "\n",
    "        initializer = tf.random_uniform_initializer(minval=-0.002, maxval=0.002)\n",
    "\n",
    "        # State as input\n",
    "        state_input = tf.keras.layers.Input(shape=(self.num_states) )\n",
    "        \n",
    "        state_out_critic = tf.keras.layers.Dense(300, \n",
    "        activation=tf.keras.layers.ReLU(),\n",
    "         kernel_initializer= initializer)(state_input)\n",
    "        \n",
    "        state_out_critic = tf.keras.layers.Dense(100, \n",
    "        activation=tf.keras.layers.ReLU(),\n",
    "         kernel_initializer= initializer)(state_out_critic)\n",
    "\n",
    "        # Action as input\n",
    "        action_input = tf.keras.layers.Input(shape=(self.num_actions))\n",
    "\n",
    "        action_out_critic = tf.keras.layers.Dense(100,\n",
    "        activation=tf.keras.layers.ReLU(),\n",
    "         kernel_initializer= initializer)(action_input)\n",
    "\n",
    "        # Concatening 2 networks\n",
    "        concat = tf.keras.layers.Concatenate()(\n",
    "            [state_out_critic, action_out_critic]\n",
    "            )\n",
    "\n",
    "        out = tf.keras.layers.Dense(256, \n",
    "            activation=tf.keras.layers.ReLU(),\n",
    "            kernel_initializer= initializer)(concat)\n",
    "        \n",
    "        # Predicted Q(s,a)\n",
    "        outputs = tf.keras.layers.Dense(1)(out)\n",
    "\n",
    "        return tf.keras.Model([state_input, action_input], outputs)\n",
    "\n",
    "    def initializer(self):\n",
    "        global n\n",
    "\n",
    "        n = 0\n",
    "        actor_model = self.Actor()\n",
    "        critic_model = self.Critic()\n",
    "\n",
    "        target_actor = self.Actor()\n",
    "        target_critic = self.Critic()\n",
    "\n",
    "        critic_lr = self.CRITIC_LR\n",
    "        actor_lr = self.ACTOR_LR\n",
    "\n",
    "        critic_optimizer = tf.keras.optimizers.Adam(critic_lr )\n",
    "        actor_optimizer = tf.keras.optimizers.Adam(actor_lr)\n",
    "\n",
    "\n",
    "        models = [critic_model, actor_model, target_critic, target_actor]\n",
    "        optimizers = [critic_optimizer, actor_optimizer]\n",
    "\n",
    "        return models, optimizers \n",
    "\n",
    "    def update_target(self, target_weights, weights):\n",
    "            for (a, b) in zip(target_weights, weights):\n",
    "                a.assign(b * self.TAU + a * (1 - self.TAU))   \n",
    "     \n",
    "    def update(self, models, optimizers, state, action, reward, next_state, next_action, batch):\n",
    "        \n",
    "        state = np.array(state)\n",
    "        state = state.reshape(len(state),self.num_states)\n",
    "\n",
    "        next_state = np.array(next_state)\n",
    "        next_state = next_state.reshape(len(next_state),self.num_states)\n",
    "\n",
    "\n",
    "        action = np.float32(action)\n",
    "        action = action.reshape(len(action),self.num_actions)\n",
    "\n",
    "        next_action = np.float32(next_action)\n",
    "        next_action = next_action.reshape(len(next_action),self.num_actions)\n",
    "\n",
    "        reward = np.float32(reward)\n",
    "        reward = reward.reshape(len(reward), 1)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "                        \n",
    "            target_actions = models[3](next_state, training = True)\n",
    "\n",
    "            q = models[0]([state, action], training=True)  # Q(s,a)\n",
    "\n",
    "            y = reward + self.gamma * \\\n",
    "                models[2]([next_state, target_actions],\n",
    "                             training=True)  \n",
    "            \n",
    "            critic_loss = tf.math.reduce_mean(tf.math.square(y - q))\n",
    "            \n",
    "        critic_grad = tape.gradient(\n",
    "                critic_loss, models[0].trainable_variables)\n",
    "            \n",
    "        critic_grad = [(tf.clip_by_norm(grad, 1)) for grad in critic_grad]\n",
    "\n",
    "        optimizers[0].apply_gradients(\n",
    "                zip(critic_grad, models[0].trainable_variables))\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "\n",
    "            actions = models[1](state, training=True)\n",
    "\n",
    "            critic_q = models[0]([state, actions], training=True)\n",
    "\n",
    "            actor_loss = -tf.math.reduce_mean(critic_q)\n",
    "\n",
    "        actor_grad = tape.gradient(\n",
    "                actor_loss, models[1].trainable_variables)\n",
    "            \n",
    "        actor_grad = [(tf.clip_by_norm(grad, 1)) for grad in actor_grad]\n",
    "            \n",
    "        optimizers[1].apply_gradients(\n",
    "                zip(actor_grad, models[1].trainable_variables))\n",
    "\n",
    "        self.update_target(models[3].variables, models[1].variables)\n",
    "        self.update_target(models[2].variables, models[0].variables)\n",
    "\n",
    "        return critic_loss, actor_loss\n",
    "\n",
    "    def UONoise(self, bn):\n",
    "        global n\n",
    "        n = bn + self.THETA *(self.NMEAN - bn)*self.DT + self.STD + np.sqrt(self.DT) * np.random.normal(size=self.NMEAN.shape)\n",
    "\n",
    "        return n\n",
    "\n",
    "    def make_action(self, actor_model, state, noise):\n",
    "        global n\n",
    "        #s_nn = np.array(state, dtype=np.float32)\n",
    "        #s_nn = tf.expand_dims(s_nn, 0) #prepare state for NN \n",
    "        #n = self.UONoise(n)\n",
    "\n",
    "        return np.float32(actor_model(state) + noise)\n",
    "\n",
    "    def train(self, models, optimizers, total_trajectory):\n",
    "\n",
    "        num_sample = len(total_trajectory)\n",
    "        total_rew, atl, ctl = [], [], []\n",
    "\n",
    "        if num_sample > 4:\n",
    "            if num_sample < self.BATCH:\n",
    "                print('Training without batch beacuse num of samples < batch size')\n",
    "                for i in range(num_sample):\n",
    "                    print('Learning from sample ', i+1)\n",
    "\n",
    "                    closs, aloss = self.update(models, optimizers, \n",
    "                                            total_trajectory[i][0],\n",
    "                                            total_trajectory[i][1], \n",
    "                                            total_trajectory[i][2],\n",
    "                                            total_trajectory[i][3], \n",
    "                                            total_trajectory[i][4],\n",
    "                                            0)\n",
    "                    total_rew.append(total_trajectory[i][2])\n",
    "                    atl.append(aloss)\n",
    "                    ctl.append(closs)\n",
    "            else:\n",
    "                for mb in range(num_sample//self.BATCH):\n",
    "                    mini_state0 = []\n",
    "                    mini_action0 = []\n",
    "                    mini_state1 = []\n",
    "                    mini_action1 = []\n",
    "                    mini_reward = []\n",
    "                    \n",
    "                    for sample in range(self.BATCH):\n",
    "                        mini_state0.append(total_trajectory[mb*self.BATCH:self.BATCH*(mb+1)+1][sample][0])\n",
    "                        mini_action0.append(total_trajectory[mb*self.BATCH:self.BATCH*(mb+1)+1][sample][1])\n",
    "                        mini_reward.append(total_trajectory[mb*self.BATCH:self.BATCH*(mb+1)+1][sample][2])\n",
    "                        mini_state1.append(total_trajectory[mb*self.BATCH:self.BATCH*(mb+1)+1][sample][3])\n",
    "                        mini_action1.append(total_trajectory[mb*self.BATCH:self.BATCH*(mb+1)+1][sample][4])\n",
    "                    \n",
    "                    closs, aloss = self.update(models, optimizers,\n",
    "                                                mini_state0,\n",
    "                                                mini_action0, \n",
    "                                                mini_reward,\n",
    "                                                mini_state1, \n",
    "                                                mini_action1,\n",
    "                                                1)\n",
    "                    \n",
    "                    total_rew.append(sum(mini_reward)/len(mini_reward))\n",
    "                    atl.append(aloss)\n",
    "                    ctl.append(closs)\n",
    "                    \n",
    "            avg_reward = np.mean(total_rew)\n",
    "            avg_atl = np.mean(atl)\n",
    "            avg_ctl = np.mean(ctl)\n",
    "            print(\"Avg reward{} , Avg actor loss is {}, Avg critic loss is {}\".format(avg_reward, avg_atl, avg_ctl))\n",
    "        else:\n",
    "            print('[WARNING] Few samples acquired. Skipping training phase. ')\n",
    "        return avg_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb8c8fc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Global variables initialization...\n",
      "[INFO] Simulation is finish. Learning for episode  1  / 1000  with 200 steps\n",
      "Avg reward-7.809515476226807 , Avg actor loss is 0.025225182995200157, Avg critic loss is 76.28607940673828\n",
      "[INFO] Simulation is finish. Learning for episode  2  / 1000  with 200 steps\n",
      "Avg reward-8.199185371398926 , Avg actor loss is 0.1087888851761818, Avg critic loss is 56.98373794555664\n",
      "[INFO] Simulation is finish. Learning for episode  3  / 1000  with 200 steps\n",
      "Avg reward-8.124225616455078 , Avg actor loss is 0.2718367278575897, Avg critic loss is 19.27635383605957\n",
      "[INFO] Simulation is finish. Learning for episode  4  / 1000  with 200 steps\n",
      "Avg reward-8.272960662841797 , Avg actor loss is 0.31478434801101685, Avg critic loss is 62.331974029541016\n",
      "[INFO] Simulation is finish. Learning for episode  5  / 1000  with 200 steps\n",
      "Avg reward-8.112679481506348 , Avg actor loss is 0.20993517339229584, Avg critic loss is 54.988773345947266\n",
      "[INFO] Simulation is finish. Learning for episode  6  / 1000  with 200 steps\n",
      "Avg reward-3.743107557296753 , Avg actor loss is 0.10632092505693436, Avg critic loss is 40.480587005615234\n",
      "[INFO] Simulation is finish. Learning for episode  7  / 1000  with 200 steps\n",
      "Avg reward-7.45675802230835 , Avg actor loss is 0.06332124769687653, Avg critic loss is 11.30699634552002\n",
      "[INFO] Simulation is finish. Learning for episode  8  / 1000  with 200 steps\n",
      "Avg reward-6.427608013153076 , Avg actor loss is 0.0764658972620964, Avg critic loss is 8.7317476272583\n",
      "[INFO] Simulation is finish. Learning for episode  9  / 1000  with 200 steps\n",
      "Avg reward-8.237105369567871 , Avg actor loss is 0.08581426739692688, Avg critic loss is 15.64364242553711\n",
      "[INFO] Simulation is finish. Learning for episode  10  / 1000  with 200 steps\n",
      "Avg reward-6.495761871337891 , Avg actor loss is 0.06533393263816833, Avg critic loss is 7.170684337615967\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from stable_baselines.common import make_vec_env\n",
    "\n",
    "envs = make_vec_env('Pendulum-v1')\n",
    "\n",
    "print('[INFO] Global variables initialization...')\n",
    "\n",
    "total_trajectory, tr = [], []\n",
    "\n",
    "agents = DDPG()\n",
    "agents.num_states = 3\n",
    "agents.num_actions = 1\n",
    "agents.max_effort = 2\n",
    "agents.gamma = 0.99\n",
    "agents.std = 0.3\n",
    "agents.ALC = 1e-3\n",
    "agents.CLR = 1e-3\n",
    "agents.TAU = 0.001\n",
    "agents.BATCH = 32\n",
    "agents.TEHTA = 0.15\n",
    "models, optimizers = agents.initializer()\n",
    "\n",
    "\n",
    "episode, step, done = 0 , 0, False\n",
    "total_trajectory = []\n",
    "\n",
    "def exp_decay(epoch, ini_value, decay):\n",
    "\n",
    "    new_value = ini_value * np.exp(-decay*epoch)\n",
    "\n",
    "    return new_value\n",
    "\n",
    "while episode < MAX_EPISODE:\n",
    "    state = envs.reset()\n",
    "    noise = agents.UONoise(0)\n",
    "    while done == False:\n",
    "        action = agents.make_action(models[1], state, noise)\n",
    "        next_state, reward, done, _ = envs.step(action)\n",
    "        noise = agents.UONoise(noise)\n",
    "        next_action = agents.make_action(models[1], next_state, noise)\n",
    "        total_trajectory.append([state, action, reward, next_state, next_action])\n",
    "        envs.render()\n",
    "        step += 1\n",
    "        noise = agents.UONoise(noise)\n",
    "        optimizers[0].lr = exp_decay(episode, agents.CRITIC_LR, 1//MAX_EPISODE)\n",
    "        optimizers[1].lr = exp_decay(episode, agents.ACTOR_LR,  1//MAX_EPISODE)\n",
    "        if done:\n",
    "            print('[INFO] Simulation is finish. Learning for episode ', episode+1, ' /', MAX_EPISODE, ' with {} steps'.format(step))\n",
    "            agents.train(models, optimizers, total_trajectory)\n",
    "            optimizers[0].lr = exp_decay(episode, agents.CRITIC_LR, 1//MAX_EPISODE)\n",
    "            optimizers[1].lr = exp_decay(episode, agents.ACTOR_LR,  1//MAX_EPISODE)\n",
    "            step, done, total_trajectory = 0 , False, []\n",
    "            episode += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a5487b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbdb246",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
