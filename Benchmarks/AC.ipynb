{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "989375e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-20 09:09:17.192491: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/giulio/rl_prj/devel/lib:/home/giulio/project_ws/devel/lib:/opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu\n",
      "2022-04-20 09:09:17.192549: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "\n",
    "class AC():\n",
    "\n",
    "    def __init__(self, GAMMA=0.99, UB=1, LB = -1, CLR = 1e-3, ALR = 1e-3, BATCH = 1):\n",
    "\n",
    "        self.num_states = 3\n",
    "        self.num_actions = 1\n",
    "        self.GAMMA = GAMMA\n",
    "        self.UPPER_BOUND = UB\n",
    "        self.LOWER_BOUND = LB\n",
    "        self.CRITIC_LR = CLR\n",
    "        self.ACTOR_LR = ALR\n",
    "\n",
    "    def getActor(self):\n",
    "        print('Creating actor..')\n",
    "        \n",
    "        \n",
    "        inputs = tf.keras.layers.Input(shape=(self.num_states,))\n",
    "        \n",
    "        out = tf.keras.layers.Dense(1024, activation=\"tanh\")(inputs)\n",
    "        out = tf.keras.layers.Dense(1024, activation=\"relu\")(out)\n",
    "        \n",
    "        \n",
    "        outputs = tf.keras.layers.Dense(self.num_actions,\n",
    "                                       kernel_initializer = tf.keras.initializers.HeNormal())(out)\n",
    "\n",
    "        model = tf.keras.Model(inputs, outputs)\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def getCritic(self):\n",
    "        \n",
    "        print('Creating critic..')\n",
    "        state_input = tf.keras.layers.Input(shape=(self.num_states))\n",
    "        \n",
    "        state_out = tf.keras.layers.Dense(1024, activation=\"relu\")(state_input)\n",
    "        state_out = tf.keras.layers.Dense(1024, activation=\"relu\")(state_out)\n",
    "\n",
    "        outputs = tf.keras.layers.Dense(1)(state_out)\n",
    "\n",
    "        model = tf.keras.Model(state_input, outputs)\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    \n",
    "    def make_action(self, actor_model, observations):\n",
    "          \n",
    "        logits = actor_model(observations)\n",
    "                \n",
    "        dist = tfp.distributions.Categorical(logits)\n",
    "        \n",
    "        action = dist.sample()\n",
    "                \n",
    "        return action[0].numpy()\n",
    "    \n",
    "\n",
    "    def initialize(self):\n",
    "        \n",
    "        print('Initializing..')\n",
    "        \n",
    "        actor = self.getActor()\n",
    "        \n",
    "        critic = self.getCritic()\n",
    "        \n",
    "        critic_optimizer = tf.keras.optimizers.RMSprop(self.CRITIC_LR)\n",
    "        \n",
    "        actor_optimizer = tf.keras.optimizers.RMSprop(self.ACTOR_LR)\n",
    "        \n",
    "        self.getBuffer()\n",
    "        \n",
    "        return  [actor, critic, critic_optimizer, actor_optimizer]\n",
    "    \n",
    "    def getBuffer(self):\n",
    "        self.buffer_counter = 0\n",
    "\n",
    "        self.states_buffer = []\n",
    "        self.actions_buffer = []\n",
    "        self.rewards_buffer = []\n",
    "        self.next_states_buffer = []\n",
    "        self.G_buffer = []\n",
    "        self.dones_buffer = []\n",
    "        \n",
    "\n",
    "    def record(self, obs_tuple):\n",
    "\n",
    "        self.states_buffer.append(obs_tuple[0])\n",
    "        self.actions_buffer.append(obs_tuple[1])\n",
    "        self.rewards_buffer.append(obs_tuple[2])\n",
    "        self.next_states_buffer.append(obs_tuple[3])\n",
    "        self.dones_buffer.append(obs_tuple[4])\n",
    "\n",
    "        self.buffer_counter += 1\n",
    "        \n",
    "    def forget(self):\n",
    "        \n",
    "        self.states_buffer = []\n",
    "        self.actions_buffer = []\n",
    "        self.rewards_buffer = []\n",
    "        self.G_buffer = []\n",
    "        self.next_states_buffer = []\n",
    "        self.dones_buffer = []\n",
    "        \n",
    "        self.buffer_counter = 0\n",
    "    \n",
    "\n",
    "    def process_rewards(self):\n",
    "        G = []\n",
    "        total_r = 0\n",
    "\n",
    "        #iterate rewards from Gt to G0\n",
    "        for r in reversed(self.rewards_buffer):\n",
    "            total_r = r + total_r * self.GAMMA\n",
    "            G.insert(0, total_r)\n",
    "\n",
    "        #whitening rewards\n",
    "        self.G_buffer = (G - np.array(G).mean())/np.array(G).std()\n",
    "        \n",
    "    \n",
    "    def update(self, actor_model, critic_model, actor_optimizer, critic_optimizer,\n",
    "               state_batch, action_batch, reward_batch, next_state_batch, done_batch):\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            \n",
    "            b = critic_model(state_batch, training = True)\n",
    "                        \n",
    "            b1 = critic_model(next_state_batch, training = True)\n",
    "            \n",
    "            y = reward_batch + self.GAMMA *b1*(1-done_batch)\n",
    "                        \n",
    "            critic_loss = tf.math.reduce_mean(tf.math.square(tf.convert_to_tensor(y - b)))\n",
    "            \n",
    "        critic_grad = tape.gradient(critic_loss, critic_model.trainable_variables)\n",
    "\n",
    "        critic_optimizer.apply_gradients(\n",
    "            zip(critic_grad, critic_model.trainable_variables)\n",
    "        )        \n",
    "    \n",
    "        with tf.GradientTape() as tape:\n",
    "            \n",
    "            logits = actor_model(state_batch, training = True)\n",
    "                        \n",
    "            b = critic_model(state_batch, training = True)            \n",
    "        \n",
    "            log_probs =  tfp.distributions.Categorical(logits).log_prob(action_batch)\n",
    "                                    \n",
    "            actor_loss = tf.math.reduce_mean(-log_probs * tf.convert_to_tensor(b - reward_batch))\n",
    "                        \n",
    "        actor_grad = tape.gradient(actor_loss, actor_model.trainable_variables)\n",
    "            \n",
    "        actor_optimizer.apply_gradients(\n",
    "            zip(actor_grad, actor_model.trainable_variables)\n",
    "        )\n",
    "\n",
    "    def learn(self, actor_model, critic_model, actor_optimizer, critic_optimizer):    \n",
    "        \n",
    "        #self.process_rewards()\n",
    "        \n",
    "        state_batch = tf.convert_to_tensor(self.states_buffer)\n",
    "        action_batch = tf.convert_to_tensor(self.actions_buffer)\n",
    "        reward_batch = tf.convert_to_tensor(self.rewards_buffer)\n",
    "        next_state_batch = tf.convert_to_tensor(self.next_states_buffer)\n",
    "        done_batch = tf.convert_to_tensor(self.dones_buffer)\n",
    "        \n",
    "        reward_batch = tf.cast(reward_batch, dtype=tf.float32)\n",
    "        done_batch = tf.cast(done_batch, dtype=tf.float32)\n",
    "\n",
    "        self.update(actor_model, critic_model, actor_optimizer, critic_optimizer, \n",
    "                    state_batch, action_batch, reward_batch, next_state_batch, done_batch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e14f09f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing..\n",
      "Creating actor..\n",
      "Creating critic..\n",
      "Initializing networks...\n",
      "Episode 1 is finish. The avg reward-to-go is -187.0 \n",
      "Episode 2 is finish. The avg reward-to-go is -167.0 \n",
      "Episode 3 is finish. The avg reward-to-go is -192.0 \n",
      "Episode 4 is finish. The avg reward-to-go is -191.0 \n",
      "Episode 5 is finish. The avg reward-to-go is -192.0 \n",
      "Episode 6 is finish. The avg reward-to-go is -193.0 \n",
      "Episode 7 is finish. The avg reward-to-go is -186.0 \n",
      "Episode 8 is finish. The avg reward-to-go is -184.0 \n",
      "Episode 9 is finish. The avg reward-to-go is -188.0 \n",
      "Episode 10 is finish. The avg reward-to-go is -189.0 \n",
      "Episode 11 is finish. The avg reward-to-go is -186.0 \n",
      "Episode 12 is finish. The avg reward-to-go is -186.0 \n",
      "Episode 13 is finish. The avg reward-to-go is -192.0 \n",
      "Episode 14 is finish. The avg reward-to-go is -184.0 \n",
      "Episode 15 is finish. The avg reward-to-go is -179.0 \n",
      "Episode 16 is finish. The avg reward-to-go is -188.0 \n",
      "Episode 17 is finish. The avg reward-to-go is -187.0 \n",
      "Episode 18 is finish. The avg reward-to-go is -183.0 \n",
      "Episode 19 is finish. The avg reward-to-go is -193.0 \n",
      "Episode 20 is finish. The avg reward-to-go is -187.0 \n",
      "Episode 21 is finish. The avg reward-to-go is -189.0 \n",
      "Episode 22 is finish. The avg reward-to-go is -192.0 \n",
      "Episode 23 is finish. The avg reward-to-go is -191.0 \n",
      "Episode 24 is finish. The avg reward-to-go is -186.0 \n",
      "Episode 25 is finish. The avg reward-to-go is -193.0 \n",
      "Episode 26 is finish. The avg reward-to-go is -190.0 \n",
      "Episode 27 is finish. The avg reward-to-go is -65.0 \n",
      "Episode 28 is finish. The avg reward-to-go is -107.0 \n",
      "Episode 29 is finish. The avg reward-to-go is -106.0 \n",
      "Episode 30 is finish. The avg reward-to-go is -172.0 \n",
      "Episode 31 is finish. The avg reward-to-go is -125.0 \n",
      "Episode 32 is finish. The avg reward-to-go is -165.0 \n",
      "Episode 33 is finish. The avg reward-to-go is -45.0 \n",
      "Episode 34 is finish. The avg reward-to-go is -105.0 \n",
      "Episode 35 is finish. The avg reward-to-go is -120.0 \n",
      "Episode 36 is finish. The avg reward-to-go is -133.0 \n",
      "Episode 37 is finish. The avg reward-to-go is -122.0 \n",
      "Episode 38 is finish. The avg reward-to-go is -114.0 \n",
      "Episode 39 is finish. The avg reward-to-go is -177.0 \n",
      "Episode 40 is finish. The avg reward-to-go is -43.0 \n",
      "Episode 41 is finish. The avg reward-to-go is -176.0 \n",
      "Episode 42 is finish. The avg reward-to-go is -130.0 \n",
      "Episode 43 is finish. The avg reward-to-go is -181.0 \n",
      "Episode 44 is finish. The avg reward-to-go is -89.0 \n",
      "Episode 45 is finish. The avg reward-to-go is -143.0 \n",
      "Episode 46 is finish. The avg reward-to-go is -152.0 \n",
      "Episode 47 is finish. The avg reward-to-go is -192.0 \n",
      "Episode 48 is finish. The avg reward-to-go is -144.0 \n",
      "Episode 49 is finish. The avg reward-to-go is -144.0 \n",
      "Episode 50 is finish. The avg reward-to-go is -165.0 \n",
      "Episode 51 is finish. The avg reward-to-go is -176.0 \n",
      "Episode 52 is finish. The avg reward-to-go is -188.0 \n",
      "Episode 53 is finish. The avg reward-to-go is -190.0 \n",
      "Episode 54 is finish. The avg reward-to-go is -191.0 \n",
      "Episode 55 is finish. The avg reward-to-go is -190.0 \n",
      "Episode 56 is finish. The avg reward-to-go is -186.0 \n",
      "Episode 57 is finish. The avg reward-to-go is -189.0 \n",
      "Episode 58 is finish. The avg reward-to-go is -182.0 \n",
      "Episode 59 is finish. The avg reward-to-go is -191.0 \n",
      "Episode 60 is finish. The avg reward-to-go is -168.0 \n",
      "Episode 61 is finish. The avg reward-to-go is -192.0 \n",
      "Episode 62 is finish. The avg reward-to-go is -189.0 \n",
      "Episode 63 is finish. The avg reward-to-go is -190.0 \n",
      "Episode 64 is finish. The avg reward-to-go is -189.0 \n",
      "Episode 65 is finish. The avg reward-to-go is -178.0 \n",
      "Episode 66 is finish. The avg reward-to-go is -169.0 \n",
      "Episode 67 is finish. The avg reward-to-go is -189.0 \n",
      "Episode 68 is finish. The avg reward-to-go is -192.0 \n",
      "Episode 69 is finish. The avg reward-to-go is -174.0 \n",
      "Episode 70 is finish. The avg reward-to-go is -192.0 \n",
      "Episode 71 is finish. The avg reward-to-go is -160.0 \n",
      "Episode 72 is finish. The avg reward-to-go is -150.0 \n",
      "Episode 73 is finish. The avg reward-to-go is -112.0 \n",
      "Episode 74 is finish. The avg reward-to-go is -188.0 \n",
      "Episode 75 is finish. The avg reward-to-go is -88.0 \n",
      "Episode 76 is finish. The avg reward-to-go is -161.0 \n",
      "Episode 77 is finish. The avg reward-to-go is -21.0 \n",
      "Episode 78 is finish. The avg reward-to-go is -112.0 \n",
      "Episode 79 is finish. The avg reward-to-go is -143.0 \n",
      "Episode 80 is finish. The avg reward-to-go is -147.0 \n",
      "Episode 81 is finish. The avg reward-to-go is -168.0 \n",
      "Episode 82 is finish. The avg reward-to-go is -140.0 \n",
      "Episode 83 is finish. The avg reward-to-go is -40.0 \n",
      "Episode 84 is finish. The avg reward-to-go is -164.0 \n",
      "Episode 85 is finish. The avg reward-to-go is -83.0 \n",
      "Episode 86 is finish. The avg reward-to-go is -136.0 \n",
      "Episode 87 is finish. The avg reward-to-go is -139.0 \n",
      "Episode 88 is finish. The avg reward-to-go is -72.0 \n",
      "Episode 89 is finish. The avg reward-to-go is -138.0 \n",
      "Episode 90 is finish. The avg reward-to-go is -147.0 \n",
      "Episode 91 is finish. The avg reward-to-go is -75.0 \n",
      "Episode 92 is finish. The avg reward-to-go is -136.0 \n",
      "Episode 93 is finish. The avg reward-to-go is -155.0 \n",
      "Episode 94 is finish. The avg reward-to-go is -132.0 \n",
      "Episode 95 is finish. The avg reward-to-go is -68.0 \n",
      "Episode 96 is finish. The avg reward-to-go is -75.0 \n",
      "Episode 97 is finish. The avg reward-to-go is -91.0 \n",
      "Episode 98 is finish. The avg reward-to-go is -139.0 \n",
      "Episode 99 is finish. The avg reward-to-go is -159.0 \n",
      "Episode 100 is finish. The avg reward-to-go is -161.0 \n",
      "Episode 101 is finish. The avg reward-to-go is -164.0 \n",
      "Episode 102 is finish. The avg reward-to-go is -178.0 \n",
      "Episode 103 is finish. The avg reward-to-go is -170.0 \n",
      "Episode 104 is finish. The avg reward-to-go is -132.0 \n",
      "Episode 105 is finish. The avg reward-to-go is -141.0 \n",
      "Episode 106 is finish. The avg reward-to-go is -174.0 \n",
      "Episode 107 is finish. The avg reward-to-go is -177.0 \n",
      "Episode 108 is finish. The avg reward-to-go is 112.0 \n",
      "Episode 109 is finish. The avg reward-to-go is -134.0 \n",
      "Episode 110 is finish. The avg reward-to-go is -84.0 \n",
      "Episode 111 is finish. The avg reward-to-go is -141.0 \n",
      "Episode 112 is finish. The avg reward-to-go is -133.0 \n",
      "Episode 113 is finish. The avg reward-to-go is -67.0 \n",
      "Episode 114 is finish. The avg reward-to-go is 299.0 \n",
      "Episode 115 is finish. The avg reward-to-go is -129.0 \n",
      "Episode 116 is finish. The avg reward-to-go is -39.0 \n",
      "Episode 117 is finish. The avg reward-to-go is -115.0 \n",
      "Episode 118 is finish. The avg reward-to-go is -56.0 \n",
      "Episode 119 is finish. The avg reward-to-go is -57.0 \n",
      "Episode 120 is finish. The avg reward-to-go is -96.0 \n",
      "Episode 121 is finish. The avg reward-to-go is -143.0 \n",
      "Episode 122 is finish. The avg reward-to-go is -95.0 \n",
      "Episode 123 is finish. The avg reward-to-go is -137.0 \n",
      "Episode 124 is finish. The avg reward-to-go is -106.0 \n",
      "Episode 125 is finish. The avg reward-to-go is -102.0 \n",
      "Episode 126 is finish. The avg reward-to-go is -45.0 \n",
      "Episode 127 is finish. The avg reward-to-go is -91.0 \n",
      "Episode 128 is finish. The avg reward-to-go is -95.0 \n",
      "Episode 129 is finish. The avg reward-to-go is -133.0 \n",
      "Episode 130 is finish. The avg reward-to-go is -110.0 \n",
      "Episode 131 is finish. The avg reward-to-go is 22.0 \n",
      "Episode 132 is finish. The avg reward-to-go is -80.0 \n",
      "Episode 133 is finish. The avg reward-to-go is -118.0 \n",
      "Episode 134 is finish. The avg reward-to-go is -19.0 \n",
      "Episode 135 is finish. The avg reward-to-go is -122.0 \n",
      "Episode 136 is finish. The avg reward-to-go is -4.0 \n",
      "Episode 137 is finish. The avg reward-to-go is 29.0 \n",
      "Episode 138 is finish. The avg reward-to-go is 299.0 \n",
      "Episode 139 is finish. The avg reward-to-go is -30.0 \n",
      "Episode 140 is finish. The avg reward-to-go is -117.0 \n",
      "Episode 141 is finish. The avg reward-to-go is -95.0 \n",
      "Episode 142 is finish. The avg reward-to-go is -137.0 \n",
      "Episode 143 is finish. The avg reward-to-go is -104.0 \n",
      "Episode 144 is finish. The avg reward-to-go is -34.0 \n",
      "Episode 145 is finish. The avg reward-to-go is -39.0 \n",
      "Episode 146 is finish. The avg reward-to-go is -80.0 \n",
      "Episode 147 is finish. The avg reward-to-go is 261.0 \n",
      "Episode 148 is finish. The avg reward-to-go is -49.0 \n",
      "Episode 149 is finish. The avg reward-to-go is 21.0 \n",
      "Episode 150 is finish. The avg reward-to-go is -37.0 \n",
      "Episode 151 is finish. The avg reward-to-go is -53.0 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 152 is finish. The avg reward-to-go is -47.0 \n",
      "Episode 153 is finish. The avg reward-to-go is 118.0 \n",
      "Episode 154 is finish. The avg reward-to-go is 45.0 \n",
      "Episode 155 is finish. The avg reward-to-go is 299.0 \n",
      "Episode 156 is finish. The avg reward-to-go is 273.0 \n",
      "Episode 157 is finish. The avg reward-to-go is 299.0 \n",
      "Episode 158 is finish. The avg reward-to-go is 117.0 \n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "\n",
    "problem = 'CartPole-v1'\n",
    "env = gym.make(problem)\n",
    "LOAD = False\n",
    "\n",
    "agent = AC(ALR = 1e-3, CLR = 1e-3 , GAMMA = 0.99)\n",
    "agent.num_states = 4\n",
    "agent.num_actions = 2\n",
    "agent.BATCH = 1\n",
    "\n",
    "actor_model, critic_model, critic_optimizer, actor_optimizer = agent.initialize()\n",
    "\n",
    "MAX_EPISODES = 1000\n",
    "\n",
    "Gt, score, ep= 0, 0, 0\n",
    "scores = []\n",
    "\n",
    "if LOAD == True:\n",
    "    print('Loading last weights')\n",
    "    actor_model.load_weights('cartpole_actor.h5')\n",
    "    critic_model.load_weights('cartpole_critic.h5')\n",
    "else:\n",
    "    print('Initializing networks...')\n",
    "\n",
    "while ep < MAX_EPISODES:\n",
    "    state = env.reset()\n",
    "    Gt, step = 0, 0\n",
    "    agent.forget()\n",
    "    while True:\n",
    "        tf_prev_state = tf.expand_dims(tf.convert_to_tensor(state), 0)\n",
    "                \n",
    "        action = agent.make_action(actor_model, tf_prev_state)\n",
    "        \n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        if done:\n",
    "            Gt += -200\n",
    "        else:\n",
    "            Gt += reward\n",
    "            \n",
    "        agent.record([state, action, Gt, next_state, done])\n",
    "\n",
    "        env.render()\n",
    "        \n",
    "        step += 1\n",
    "        if done: \n",
    "            ep += 1\n",
    "            agent.learn(actor_model, critic_model, critic_optimizer, actor_optimizer)\n",
    "            print(\"Episode {} is finish. The avg reward-to-go is {} \".format(ep, Gt))\n",
    "            last = 0\n",
    "            break\n",
    "            \n",
    "        state = next_state\n",
    "\n",
    "print('Learning is completed. Saving results..')\n",
    "actor_model.save_weights(\"cartpole_actor.h5\")\n",
    "critic_model.save_weights(\"cartpole_critic.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96283ada",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d934a94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
