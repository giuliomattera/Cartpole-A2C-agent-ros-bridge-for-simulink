{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ab08cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SARSA():\n",
    "\n",
    "    def __init__(self, UB=1, LB = -1, LR = 1e-3, EPS = 0.4, GAMMA = 0.99):\n",
    "\n",
    "        super(SARSA, self).__init__()\n",
    "        self.num_states = 3\n",
    "        self.num_actions = 1\n",
    "        self.UPPER_BOUND = UB\n",
    "        self.LR = LR\n",
    "        self.EPS = EPS\n",
    "        self.GAMMA = GAMMA\n",
    "\n",
    "    def Critic(self):\n",
    "        state_input = tf.keras.layers.Input(shape=(self.num_states))\n",
    "        action_input = tf.keras.layers.Input(shape=(1))\n",
    "        \n",
    "        state_hid = tf.keras.layers.Dense(16, activation=\"relu\")(state_input)\n",
    "        action_hid = tf.keras.layers.Dense(16, activation=\"relu\")(action_input)\n",
    "        \n",
    "        common = tf.keras.layers.Add()([state_hid, action_hid])\n",
    "        \n",
    "        outputs = tf.keras.layers.Dense(self.num_actions)(common)\n",
    "\n",
    "        model = tf.keras.Model([state_input, action_input], outputs)\n",
    "\n",
    "        return model\n",
    "\n",
    "    def epsilon_greedy_policy(self, model, state, action, epsilon=0):\n",
    "        \n",
    "        if np.random.rand() < epsilon:\n",
    "            return np.random.randint(self.num_actions)\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            Q_values = model([state,action])\n",
    "            \n",
    "            return np.argmax(Q_values)\n",
    "\n",
    "    def initialize(self):\n",
    "        \n",
    "        critic = self.Critic()\n",
    "        \n",
    "        critic_optimizer = tf.keras.optimizers.Adam(self.LR)\n",
    "\n",
    "        \n",
    "        return critic, critic_optimizer\n",
    "    \n",
    "    def update(self, model, optimizer, state, action, reward, next_state):\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            \n",
    "            q0 = model([state, action], training=True)\n",
    "            \n",
    "            q1 = model([next_state, action], training=True)\n",
    "            \n",
    "            mq0 = tf.experimental.numpy.amax(q0, axis=1)\n",
    "            \n",
    "            mq1 = tf.experimental.numpy.amax(q1, axis=1)\n",
    "            \n",
    "            y = tf.math.add(reward, self.GAMMA*mq0)\n",
    "\n",
    "            critic_loss = tf.math.reduce_mean(tf.math.square(y-mq1))\n",
    "\n",
    "        critic_grad = tape.gradient(critic_loss, model.trainable_variables)\n",
    "\n",
    "        optimizer.apply_gradients(\n",
    "            zip(critic_grad, model.trainable_variables)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c36016c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-14 14:25:12.036097: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-04-14 14:25:12.036859: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2022-04-14 14:25:12.064335: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-14 14:25:12.064685: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce GTX 1650 Ti computeCapability: 7.5\n",
      "coreClock: 1.485GHz coreCount: 16 deviceMemorySize: 3.82GiB deviceMemoryBandwidth: 178.84GiB/s\n",
      "2022-04-14 14:25:12.064854: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/giulio/.local/lib/python3.8/site-packages/cv2/../../lib64:/home/giulio/rl_prj/devel/lib:/home/giulio/project_ws/devel/lib:/opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu\n",
      "2022-04-14 14:25:12.064928: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/giulio/.local/lib/python3.8/site-packages/cv2/../../lib64:/home/giulio/rl_prj/devel/lib:/home/giulio/project_ws/devel/lib:/opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu\n",
      "2022-04-14 14:25:12.064985: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/giulio/.local/lib/python3.8/site-packages/cv2/../../lib64:/home/giulio/rl_prj/devel/lib:/home/giulio/project_ws/devel/lib:/opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu\n",
      "2022-04-14 14:25:12.066364: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2022-04-14 14:25:12.066844: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2022-04-14 14:25:12.068458: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-04-14 14:25:12.068653: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/giulio/.local/lib/python3.8/site-packages/cv2/../../lib64:/home/giulio/rl_prj/devel/lib:/home/giulio/project_ws/devel/lib:/opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu\n",
      "2022-04-14 14:25:12.068720: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/giulio/.local/lib/python3.8/site-packages/cv2/../../lib64:/home/giulio/rl_prj/devel/lib:/home/giulio/project_ws/devel/lib:/opt/ros/noetic/lib:/opt/ros/noetic/lib/x86_64-linux-gnu\n",
      "2022-04-14 14:25:12.068731: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1757] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-04-14 14:25:12.070194: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-04-14 14:25:12.070234: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2022-04-14 14:25:12.070239: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaol not reched yet. For the episode 0 the final reward-to-go is 45.0\n",
      "Gaol not reched yet. For the episode 10 the final reward-to-go is 26.0\n",
      "Gaol not reched yet. For the episode 20 the final reward-to-go is 13.0\n",
      "Gaol not reched yet. For the episode 30 the final reward-to-go is 20.0\n",
      "Gaol not reched yet. For the episode 40 the final reward-to-go is 23.0\n",
      "Gaol not reched yet. For the episode 50 the final reward-to-go is 35.0\n",
      "Gaol not reched yet. For the episode 60 the final reward-to-go is 48.0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer dense_1 is incompatible with the layer: : expected min_ndim=2, found ndim=0. Full shape received: ()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     28\u001b[0m     tf_prev_state \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mexpand_dims(tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(state), \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 30\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepsilon_greedy_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtf_prev_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     next_state, reward, done, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     34\u001b[0m     score \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36mSARSA.epsilon_greedy_policy\u001b[0;34m(self, model, state, action, epsilon)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_actions)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 35\u001b[0m     Q_values \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43maction\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39margmax(Q_values)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py:1012\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1008\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_build(inputs)\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast_variable\u001b[38;5;241m.\u001b[39menable_auto_cast_variables(\n\u001b[1;32m   1011\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_dtype_object):\n\u001b[0;32m-> 1012\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mcall_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_activity_regularizer:\n\u001b[1;32m   1015\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/functional.py:424\u001b[0m, in \u001b[0;36mFunctional.call\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    407\u001b[0m   \u001b[38;5;124;03m\"\"\"Calls the model on new inputs.\u001b[39;00m\n\u001b[1;32m    408\u001b[0m \n\u001b[1;32m    409\u001b[0m \u001b[38;5;124;03m  In this case `call` just reapplies\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;124;03m      a list of tensors if there are more than one outputs.\u001b[39;00m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 424\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_internal_graph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/functional.py:560\u001b[0m, in \u001b[0;36mFunctional._run_internal_graph\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    557\u001b[0m   \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# Node is not computable, try skipping.\u001b[39;00m\n\u001b[1;32m    559\u001b[0m args, kwargs \u001b[38;5;241m=\u001b[39m node\u001b[38;5;241m.\u001b[39mmap_arguments(tensor_dict)\n\u001b[0;32m--> 560\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;66;03m# Update tensor_dict.\u001b[39;00m\n\u001b[1;32m    563\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x_id, y \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(node\u001b[38;5;241m.\u001b[39mflat_output_ids, nest\u001b[38;5;241m.\u001b[39mflatten(outputs)):\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py:998\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    995\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_autocast:\n\u001b[1;32m    996\u001b[0m   inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[0;32m--> 998\u001b[0m \u001b[43minput_spec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_input_compatibility\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_spec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    999\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m eager:\n\u001b[1;32m   1000\u001b[0m   call_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/input_spec.py:234\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    232\u001b[0m   ndim \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;241m.\u001b[39mrank\n\u001b[1;32m    233\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m ndim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m ndim \u001b[38;5;241m<\u001b[39m spec\u001b[38;5;241m.\u001b[39mmin_ndim:\n\u001b[0;32m--> 234\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(input_index) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m    235\u001b[0m                      layer_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is incompatible with the layer: \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    236\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: expected min_ndim=\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(spec\u001b[38;5;241m.\u001b[39mmin_ndim) \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m    237\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, found ndim=\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(ndim) \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m    238\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Full shape received: \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m    239\u001b[0m                      \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mtuple\u001b[39m(shape)))\n\u001b[1;32m    240\u001b[0m \u001b[38;5;66;03m# Check dtype.\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: Input 0 of layer dense_1 is incompatible with the layer: : expected min_ndim=2, found ndim=0. Full shape received: ()"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "problem = \"CartPole-v1\"\n",
    "env = gym.make(problem)\n",
    "env.seed(42)\n",
    "\n",
    "agent = SARSA(LR = 1e-3)\n",
    "agent.num_states = 4\n",
    "agent.num_actions = 2\n",
    "\n",
    "model, optimizer = agent.initialize()\n",
    "\n",
    "MAX_EPISODES = 40000\n",
    "\n",
    "states, next_states, rewards, actions, score = [], [], [], [], 0\n",
    "\n",
    "for ep in range(MAX_EPISODES):\n",
    "    state = env.reset()\n",
    "    action = tf.expand_dims(tf.convert_to_tensor(np.array([1])), 0)\n",
    "    epsilon = max(1- ep/MAX_EPISODES, 0.01)\n",
    "    while True:\n",
    "        \n",
    "        tf_prev_state = tf.expand_dims(tf.convert_to_tensor(state), 0)\n",
    "                \n",
    "        action = agent.epsilon_greedy_policy(model, tf_prev_state, action, epsilon)\n",
    "                     \n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        score += reward\n",
    "        \n",
    "        actions.append(action)\n",
    "                \n",
    "        rewards.append(reward)\n",
    "                \n",
    "        states.append(state)\n",
    "        \n",
    "        next_states.append(next_state)\n",
    "                        \n",
    "        state = next_state\n",
    "        \n",
    "        env.render()\n",
    "        # End this episode when `done` is True\n",
    "        if done:\n",
    "            \n",
    "            agent.update(model, optimizer, \n",
    "                         tf.convert_to_tensor(states),\n",
    "                         tf.convert_to_tensor(actions),\n",
    "                             tf.cast(tf.convert_to_tensor(rewards), tf.float32),\n",
    "                             tf.convert_to_tensor(next_states))\n",
    "            \n",
    "            \n",
    "\n",
    "            if score > 190:\n",
    "                print('Goal is reched!!')\n",
    "                actor_model.save_weights(\"cart_actor.h5\")\n",
    "                critic_model.save_weights(\"cart_critic.h5\")\n",
    "            else:\n",
    "                if ep % 10 == 0:\n",
    "                    print('Gaol not reched yet. For the episode {} the final reward-to-go is {}'.format(ep, score))\n",
    "            \n",
    "            states, next_states, actions, rewards, score = [], [], [], [], 0            \n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec995bf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c4ebf8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
